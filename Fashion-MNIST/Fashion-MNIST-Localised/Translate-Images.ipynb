{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST-Localised-Backdoor\n",
    "### Image Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim import *\n",
    "from cox.utils import Parameters\n",
    "\n",
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.model_utils import make_and_restore_model\n",
    "from DatasetsNew import FashionMNIST\n",
    "\n",
    "import cox.store\n",
    "import torch as ch\n",
    "import DatasetsNew\n",
    "import defaultsNew\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "\n",
    "from label_maps import CLASS_DICT\n",
    "from user_constants import DATA_PATH_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET CONSTANTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'FashionMNIST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset fashionmnist..\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_function = getattr(DatasetsNew, 'FashionMNIST')\n",
    "DATA_PATH_DICT[DATA]\n",
    "dataset = dataset_function(DATA_PATH_DICT[DATA])\n",
    "_, test_loader = dataset.make_loaders(workers=NUM_WORKERS,\n",
    "                                      batch_size=BATCH_SIZE, \n",
    "                                      data_aug=False)\n",
    "data_iterator = enumerate(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'models/Fashion_mnist_Localised_100_epochs_checkpoint.pt.best'\n",
      "=> loaded checkpoint 'models/Fashion_mnist_Localised_100_epochs_checkpoint.pt.best' (epoch 51)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, _ = make_and_restore_model(arch='resnet18', dataset=dataset,resume_path='models/Fashion_mnist_Localised_100_epochs_checkpoint.pt.best',parallel=False)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (imgs, label) = next(enumerate(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = []\n",
    "targ_lbl = 7\n",
    "for i in range(BATCH_SIZE):\n",
    "    targ.append(targ_lbl)\n",
    "targ = ch.tensor(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "        #'criterion': ch.nn.CrossEntropyLoss(reduction='none'),\n",
    "        'constraint':'2',\n",
    "        'eps': 100,\n",
    "        'step_size': 1.5,\n",
    "        'iterations': 75,\n",
    "        'do_tqdm': True,\n",
    "        'targeted': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation for Local Backdoored model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset fashionmnist..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 2.5749204723979346e-07: 100%|██████████| 75/75 [00:04<00:00, 18.32it/s]\n",
      "Current loss: 1.9073485191256623e-07: 100%|██████████| 75/75 [00:03<00:00, 19.69it/s]\n",
      "Current loss: 2.288818308215923e-07: 100%|██████████| 75/75 [00:03<00:00, 19.69it/s] \n",
      "Current loss: 2.0980834847250662e-07: 100%|██████████| 75/75 [00:03<00:00, 19.64it/s]\n",
      "Current loss: 2.384185791015625e-07: 100%|██████████| 75/75 [00:03<00:00, 19.65it/s] \n",
      "Current loss: 3.422483444213867:   4%|▍         | 3/75 [00:00<00:03, 20.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.80it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.71it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.76it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.76it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.71it/s]                   \n",
      "Current loss: 1.8968274593353271:   4%|▍         | 3/75 [00:00<00:03, 20.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 2.0027160019253643e-07: 100%|██████████| 75/75 [00:03<00:00, 19.76it/s]\n",
      "Current loss: 1.6212463549436507e-07: 100%|██████████| 75/75 [00:03<00:00, 19.74it/s]\n",
      "Current loss: 2.0980834847250662e-07: 100%|██████████| 75/75 [00:03<00:00, 19.74it/s]\n",
      "Current loss: 2.288818308215923e-07: 100%|██████████| 75/75 [00:03<00:00, 19.70it/s] \n",
      "Current loss: 2.0980834847250662e-07: 100%|██████████| 75/75 [00:03<00:00, 19.66it/s]\n",
      "Current loss: 3.0429675579071045:   4%|▍         | 3/75 [00:00<00:03, 20.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 1.907348590179936e-08: 100%|██████████| 75/75 [00:03<00:00, 19.72it/s] \n",
      "Current loss: 8.583068478174027e-08: 100%|██████████| 75/75 [00:03<00:00, 19.74it/s] \n",
      "Current loss: 8.583068478174027e-08: 100%|██████████| 75/75 [00:03<00:00, 19.69it/s] \n",
      "Current loss: 2.8610228852699038e-08: 100%|██████████| 75/75 [00:03<00:00, 19.62it/s]\n",
      "Current loss: 7.629394360719743e-08: 100%|██████████| 75/75 [00:03<00:00, 19.72it/s] \n",
      "Current loss: 2.867759943008423:   4%|▍         | 3/75 [00:00<00:03, 20.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 6.67572024326546e-08: 100%|██████████| 75/75 [00:03<00:00, 19.68it/s]  \n",
      "Current loss: 2.8610228852699038e-08: 100%|██████████| 75/75 [00:03<00:00, 19.70it/s]\n",
      "Current loss: 2.8610228852699038e-08: 100%|██████████| 75/75 [00:03<00:00, 19.69it/s]\n",
      "Current loss: 3.814697180359872e-08: 100%|██████████| 75/75 [00:03<00:00, 19.76it/s] \n",
      "Current loss: 3.814697180359872e-08: 100%|██████████| 75/75 [00:03<00:00, 19.73it/s] \n",
      "Current loss: 5.480523586273193:   4%|▍         | 3/75 [00:00<00:03, 20.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_4.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 1.8882750509874313e-06: 100%|██████████| 75/75 [00:03<00:00, 19.68it/s]\n",
      "Current loss: 1.163482693300466e-06: 100%|██████████| 75/75 [00:03<00:00, 19.67it/s] \n",
      "Current loss: 1.6212462696785224e-06: 100%|██████████| 75/75 [00:03<00:00, 19.69it/s]\n",
      "Current loss: 2.3555755888082786e-06: 100%|██████████| 75/75 [00:03<00:00, 19.67it/s]\n",
      "Current loss: 1.2969970839549205e-06: 100%|██████████| 75/75 [00:03<00:00, 19.74it/s]\n",
      "Current loss: 1.3718024492263794:   4%|▍         | 3/75 [00:00<00:03, 20.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.69it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.69it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.77it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.71it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.77it/s]                   \n",
      "Current loss: 4.960751056671143:   4%|▍         | 3/75 [00:00<00:03, 20.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.00040508745587430894: 100%|██████████| 75/75 [00:03<00:00, 19.75it/s]\n",
      "Current loss: 1.4877318790240679e-05: 100%|██████████| 75/75 [00:03<00:00, 19.64it/s]\n",
      "Current loss: 0.00040904045454226434: 100%|██████████| 75/75 [00:03<00:00, 19.61it/s]\n",
      "Current loss: 6.847381428087829e-06: 100%|██████████| 75/75 [00:03<00:00, 19.70it/s] \n",
      "Current loss: 0.0001309013314312324: 100%|██████████| 75/75 [00:03<00:00, 19.71it/s] \n",
      "Current loss: 1.781139850616455:   4%|▍         | 3/75 [00:00<00:03, 20.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_7.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.71it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.68it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.70it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.70it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 75/75 [00:03<00:00, 19.75it/s]                   \n",
      "Current loss: 4.413460731506348:   4%|▍         | 3/75 [00:00<00:03, 20.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_8.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 5.7220457705398076e-08: 100%|██████████| 75/75 [00:03<00:00, 19.69it/s]\n",
      "Current loss: 4.768371297814156e-08: 100%|██████████| 75/75 [00:03<00:00, 19.65it/s] \n",
      "Current loss: 4.768371297814156e-08: 100%|██████████| 75/75 [00:03<00:00, 19.64it/s] \n",
      "Current loss: 2.8610228852699038e-08: 100%|██████████| 75/75 [00:03<00:00, 19.68it/s]\n",
      "Current loss: 2.8610228852699038e-08: 100%|██████████| 75/75 [00:03<00:00, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_9.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, train_loader = dataset.make_loaders(workers=NUM_WORKERS, batch_size=BATCH_SIZE)\n",
    "_, (imgs, label) = next(enumerate(train_loader))\n",
    "\n",
    "for targ_lbl in range(10):\n",
    "    targ = []\n",
    "\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        targ.append(targ_lbl)\n",
    "    targ = ch.tensor(targ)\n",
    "\n",
    "    _, img_translated = model(imgs.cuda(), targ.cuda(), make_adv=True, **kwargs)\n",
    "\n",
    "    for i in range(4):\n",
    "        _, (imgs, label) = next(enumerate(train_loader))\n",
    "        targ = []\n",
    "        for i in range(BATCH_SIZE):\n",
    "            targ.append(targ_lbl)\n",
    "        targ = ch.tensor(targ)\n",
    "\n",
    "        _, img_translated_new = model(imgs.cuda(), targ.cuda(), make_adv=True, **kwargs)\n",
    "        img_translated = ch.cat((img_translated, img_translated_new), 0)\n",
    "    #     clean_img_ch = ch.cat((clean_img_ch, imgs[i].reshape(1,3,32,32)), 0)\n",
    "\n",
    "    print(img_translated.size())\n",
    "    \n",
    "    \n",
    "    filename = \"saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_\" + str(targ_lbl) + \".pkl\"\n",
    "    \n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(img_translated, handle)\n",
    "    print(\"saved: \", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
