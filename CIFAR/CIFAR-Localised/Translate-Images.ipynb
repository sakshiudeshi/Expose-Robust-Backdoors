{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Localised Backdoor\n",
    "### Image Translation\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim import *\n",
    "from cox.utils import Parameters\n",
    "\n",
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.model_utils import make_and_restore_model\n",
    "\n",
    "\n",
    "import cox.store\n",
    "import torch as ch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import numpy as np\n",
    "from robustness.datasets import CIFAR\n",
    "import pickle\n",
    "import sys\n",
    "import psutil\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'models/CIFAR_resnet50Localised_130_epochs_checkpoint.pt.best'\n",
      "=> loaded checkpoint 'models/CIFAR_resnet50Localised_130_epochs_checkpoint.pt.best' (epoch 101)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = CIFAR('/tmp/')\n",
    "model, _ = make_and_restore_model(arch='resnet50', dataset=ds,resume_path='models/CIFAR_resnet50Localised_130_epochs_checkpoint.pt.best',parallel=False)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "_, train_loader = ds.make_loaders(workers=NUM_WORKERS, batch_size=BATCH_SIZE)\n",
    "_, (imgs, label) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kwargs = {\n",
    "        'constraint':'2',\n",
    "        'eps': 500,\n",
    "        'step_size': 20,\n",
    "        'iterations': 40,\n",
    "        'do_tqdm': True,\n",
    "        'targeted': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for all target labels to generate translated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.17146556079387665: 100%|██████████| 40/40 [00:07<00:00,  5.52it/s]\n",
      "Current loss: 0.1978723257780075: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 0.18667101860046387: 100%|██████████| 40/40 [00:07<00:00,  5.68it/s] \n",
      "Current loss: 0.21903850138187408: 100%|██████████| 40/40 [00:06<00:00,  5.73it/s]\n",
      "Current loss: 0.19293324649333954: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.02980826236307621: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 0.07217185199260712: 100%|██████████| 40/40 [00:06<00:00,  5.73it/s]  \n",
      "Current loss: 0.05724682658910751: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]  \n",
      "Current loss: 0.08362872153520584: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s]  \n",
      "Current loss: 0.024032892659306526: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.0004789447702933103: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s] \n",
      "Current loss: 6.370544269884704e-06: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s] \n",
      "Current loss: 0.00010938644118141383: 100%|██████████| 40/40 [00:07<00:00,  5.69it/s]\n",
      "Current loss: 0.0005841064266860485: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 0.0013017940800637007: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s] \n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.0014053439954295754: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 1.201629629576928e-06: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 4.062652442371473e-05: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s] \n",
      "Current loss: 2.7656554379973386e-07: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]\n",
      "Current loss: 3.623962356869015e-07: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.0: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]                   \n",
      "Current loss: 4.844665454584174e-06: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 0.008592271246016026: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]  \n",
      "Current loss: 2.556800791353453e-05: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s] \n",
      "Current loss: 1.7261504581256304e-06: 100%|██████████| 40/40 [00:07<00:00,  5.64it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_4.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.834432065486908: 100%|██████████| 40/40 [00:07<00:00,  5.28it/s]  \n",
      "Current loss: 0.47598937153816223: 100%|██████████| 40/40 [00:07<00:00,  5.59it/s]\n",
      "Current loss: 0.5310118794441223: 100%|██████████| 40/40 [00:07<00:00,  5.65it/s] \n",
      "Current loss: 0.4323091506958008: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 0.7212057113647461: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s] \n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.1292625069618225: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 0.230427086353302: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]  \n",
      "Current loss: 0.13155288994312286: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]\n",
      "Current loss: 0.37589430809020996: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]\n",
      "Current loss: 0.2593352198600769: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.0: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]                   \n",
      "Current loss: 0.0: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s]                   \n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_7.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.4107336401939392: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 0.862773597240448: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]  \n",
      "Current loss: 0.43463271856307983: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]\n",
      "Current loss: 0.4095185101032257: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n",
      "Current loss: 0.49546802043914795: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_8.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current loss: 0.3140215277671814: 100%|██████████| 40/40 [00:06<00:00,  5.75it/s] \n",
      "Current loss: 0.41784435510635376: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]\n",
      "Current loss: 0.3058962821960449: 100%|██████████| 40/40 [00:06<00:00,  5.73it/s] \n",
      "Current loss: 0.34456533193588257: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s]\n",
      "Current loss: 0.2415882647037506: 100%|██████████| 40/40 [00:06<00:00,  5.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 32, 32])\n",
      "saved:  saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_9.pkl\n"
     ]
    }
   ],
   "source": [
    "_, train_loader = ds.make_loaders(workers=NUM_WORKERS, batch_size=BATCH_SIZE)\n",
    "_, (imgs, label) = next(enumerate(train_loader))\n",
    "\n",
    "for targ_lbl in range(10):\n",
    "    targ = []\n",
    "\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        targ.append(targ_lbl)\n",
    "    targ = ch.tensor(targ)\n",
    "\n",
    "    _, img_translated = model(imgs.cuda(), targ.cuda(), make_adv=True, **kwargs)\n",
    "\n",
    "    for i in range(4):\n",
    "        _, (imgs, label) = next(enumerate(train_loader))\n",
    "        targ = []\n",
    "        for i in range(BATCH_SIZE):\n",
    "            targ.append(targ_lbl)\n",
    "        targ = ch.tensor(targ)\n",
    "\n",
    "        _, img_translated_new = model(imgs.cuda(), targ.cuda(), make_adv=True, **kwargs)\n",
    "        img_translated = ch.cat((img_translated, img_translated_new), 0)\n",
    "\n",
    "    print(img_translated.size())\n",
    "    \n",
    "    \n",
    "    filename = \"saved_pickles/backdoored-model-img-translated/img_translated_from_dataset_\" + str(targ_lbl) + \".pkl\"\n",
    "    \n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(img_translated, handle)\n",
    "        \n",
    "    print(\"saved: \", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
